\babel@toc {english}{}
\contentsline {chapter}{\numberline {1}Introduction}{2}{chapter.1}
\contentsline {section}{\numberline {1.1}What is Reinforcement Learning?}{2}{section.1.1}
\contentsline {section}{\numberline {1.2}Machine Learning}{3}{section.1.2}
\contentsline {subsection}{\numberline {1.2.1}Supervised Learning}{3}{subsection.1.2.1}
\contentsline {subsection}{\numberline {1.2.2}Unsupervised Learning}{4}{subsection.1.2.2}
\contentsline {subsection}{\numberline {1.2.3}How is Reinforcement Learning different?}{5}{subsection.1.2.3}
\contentsline {section}{\numberline {1.3}Elements of Reinforcement Learning}{5}{section.1.3}
\contentsline {section}{\numberline {1.4}Thesis outline}{7}{section.1.4}
\contentsline {chapter}{\numberline {2}The Reinforcement Learning problem}{8}{chapter.2}
\contentsline {section}{\numberline {2.1}Markov Decision Processes (MDP)}{8}{section.2.1}
\contentsline {subsection}{\numberline {2.1.1}The Agent-Environment Interface}{8}{subsection.2.1.1}
\contentsline {subsection}{\numberline {2.1.2}The Markov Property}{9}{subsection.2.1.2}
\contentsline {subsection}{\numberline {2.1.3}Returns and Rewarding Process}{9}{subsection.2.1.3}
\contentsline {subsection}{\numberline {2.1.4}Policies and Value Functions}{10}{subsection.2.1.4}
\contentsline {subsection}{\numberline {2.1.5}Bellman Equations}{11}{subsection.2.1.5}
\contentsline {subsection}{\numberline {2.1.6}Optimality Conditions}{11}{subsection.2.1.6}
\contentsline {subsection}{\numberline {2.1.7}Exploration and Exploitation}{12}{subsection.2.1.7}
\contentsline {section}{\numberline {2.2}Dynamic Programming}{12}{section.2.2}
\contentsline {subsection}{\numberline {2.2.1}Dynamic Programming Shortcomings}{13}{subsection.2.2.1}
\contentsline {subsection}{\numberline {2.2.2}Generalized Policy Iteration}{13}{subsection.2.2.2}
\contentsline {section}{\numberline {2.3}Monte Carlo Methods}{14}{section.2.3}
\contentsline {section}{\numberline {2.4}Temporal Difference Methods}{14}{section.2.4}
\contentsline {subsection}{\numberline {2.4.1}SARSA}{15}{subsection.2.4.1}
\contentsline {subsection}{\numberline {2.4.2}Q-Learning}{16}{subsection.2.4.2}
\contentsline {chapter}{\numberline {3}Deep Neural Networks}{18}{chapter.3}
\contentsline {section}{\numberline {3.1}Why Neural Networks?}{18}{section.3.1}
\contentsline {section}{\numberline {3.2}Building Units}{18}{section.3.2}
\contentsline {subsection}{\numberline {3.2.1}Artificial Neuron}{19}{subsection.3.2.1}
\contentsline {subsection}{\numberline {3.2.2}Activation Functions}{19}{subsection.3.2.2}
\contentsline {section}{\numberline {3.3}Feed Forward Networks}{20}{section.3.3}
\contentsline {subsection}{\numberline {3.3.1}Training the Network}{21}{subsection.3.3.1}
\contentsline {subsection}{\numberline {3.3.2}Underfitting and Overfitting}{22}{subsection.3.3.2}
\contentsline {section}{\numberline {3.4}Convolutional Neural Networks}{22}{section.3.4}
\contentsline {subsection}{\numberline {3.4.1}CNN Architecture}{23}{subsection.3.4.1}
\contentsline {subsection}{\numberline {3.4.2}Applications}{24}{subsection.3.4.2}
\contentsline {chapter}{\numberline {4}Deep Reinforcement Learning}{25}{chapter.4}
\contentsline {section}{\numberline {4.1}Introduction}{25}{section.4.1}
\contentsline {section}{\numberline {4.2}Deep Q-Learning}{25}{section.4.2}
\contentsline {subsection}{\numberline {4.2.1}DQN shortcomings}{27}{subsection.4.2.1}
\contentsline {chapter}{\numberline {5}Cart-Pole Balancing with Visual Input}{28}{chapter.5}
\contentsline {section}{\numberline {5.1}Why Cart-Pole Balancing?}{28}{section.5.1}
\contentsline {section}{\numberline {5.2}Experimental Setup and Tools}{29}{section.5.2}
\contentsline {subsection}{\numberline {5.2.1}Hardware and Software}{29}{subsection.5.2.1}
\contentsline {subsection}{\numberline {5.2.2}PyTorch}{29}{subsection.5.2.2}
\contentsline {subsection}{\numberline {5.2.3}OpenAI Gym}{30}{subsection.5.2.3}
\contentsline {subsection}{\numberline {5.2.4}The CartPole Environment}{30}{subsection.5.2.4}
\contentsline {section}{\numberline {5.3}Solving the Sensor-Based CartPole}{32}{section.5.3}
\contentsline {section}{\numberline {5.4}Towards Vision-Based Control}{33}{section.5.4}
\contentsline {subsection}{\numberline {5.4.1}Improving Image Pre-Processing}{35}{subsection.5.4.1}
\contentsline {subsection}{\numberline {5.4.2}Dealing with Overfitting}{36}{subsection.5.4.2}
\contentsline {subsection}{\numberline {5.4.3}Reward Function Design}{36}{subsection.5.4.3}
\contentsline {section}{\numberline {5.5}Vision-Based Control}{37}{section.5.5}
\contentsline {subsection}{\numberline {5.5.1}Convolutional Neural Network Design}{37}{subsection.5.5.1}
\contentsline {subsection}{\numberline {5.5.2}Hyper-Parameters Settings}{38}{subsection.5.5.2}
\contentsline {subsection}{\numberline {5.5.3}Final Results}{39}{subsection.5.5.3}
\contentsline {chapter}{\numberline {6}Conclusions and Future Work}{41}{chapter.6}
\contentsline {section}{\numberline {6.1}Thesis Summary}{41}{section.6.1}
\contentsline {section}{\numberline {6.2}Future Work}{42}{section.6.2}
\contentsline {chapter}{\numberline {A}Additional Reinforcement Learning Algorithms}{44}{appendix.A}
\contentsline {section}{\numberline {A.1}Model-Free Methods}{44}{section.A.1}
\contentsline {subsection}{\numberline {A.1.1}TD($\lambda $)}{44}{subsection.A.1.1}
\contentsline {subsection}{\numberline {A.1.2}Policy Search Methods}{45}{subsection.A.1.2}
\contentsline {subsection}{\numberline {A.1.3}Actor Critic Methods}{46}{subsection.A.1.3}
\contentsline {subsection}{\numberline {A.1.4}Deterministic Policy Gradient}{46}{subsection.A.1.4}
\contentsline {section}{\numberline {A.2}Model-based Methods}{47}{section.A.2}
\contentsline {chapter}{\numberline {B}Additional Deep Reinforcement Learning Algorithms}{48}{appendix.B}
\contentsline {section}{\numberline {B.1}Deep Deterministic Policy Gradient}{48}{section.B.1}
\contentsline {section}{\numberline {B.2}Trust Region Policy Optimization}{50}{section.B.2}
\contentsline {section}{\numberline {B.3}Other Deep RL Algorithms}{50}{section.B.3}
\contentsline {subsection}{\numberline {B.3.1}Generalized Advantage Estimation}{51}{subsection.B.3.1}
\contentsline {subsection}{\numberline {B.3.2}Guided Policy Search}{51}{subsection.B.3.2}
\contentsline {section}{\numberline {B.4}Deep RL Algorithms Comparison}{51}{section.B.4}
