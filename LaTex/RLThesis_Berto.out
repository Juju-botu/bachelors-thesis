\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.1.1}{What is Reinforcement Learning?}{chapter.1}% 2
\BOOKMARK [1][-]{section.1.2}{Machine Learning}{chapter.1}% 3
\BOOKMARK [2][-]{subsection.1.2.1}{Supervised Learning}{section.1.2}% 4
\BOOKMARK [2][-]{subsection.1.2.2}{Unsupervised Learning}{section.1.2}% 5
\BOOKMARK [2][-]{subsection.1.2.3}{How is Reinforcement Learning different?}{section.1.2}% 6
\BOOKMARK [1][-]{section.1.3}{Elements of Reinforcement Learning}{chapter.1}% 7
\BOOKMARK [1][-]{section.1.4}{Thesis outline}{chapter.1}% 8
\BOOKMARK [0][-]{chapter.2}{The Reinforcement Learning problem}{}% 9
\BOOKMARK [1][-]{section.2.1}{Markov Decision Processes \(MDP\)}{chapter.2}% 10
\BOOKMARK [2][-]{subsection.2.1.1}{The Agent-Environment Interface}{section.2.1}% 11
\BOOKMARK [2][-]{subsection.2.1.2}{The Markov Property}{section.2.1}% 12
\BOOKMARK [2][-]{subsection.2.1.3}{Returns and Rewarding Process}{section.2.1}% 13
\BOOKMARK [2][-]{subsection.2.1.4}{Policies and Value Functions}{section.2.1}% 14
\BOOKMARK [2][-]{subsection.2.1.5}{Bellman Equations}{section.2.1}% 15
\BOOKMARK [2][-]{subsection.2.1.6}{Optimality Conditions}{section.2.1}% 16
\BOOKMARK [2][-]{subsection.2.1.7}{Exploration and Exploitation}{section.2.1}% 17
\BOOKMARK [1][-]{section.2.2}{Dynamic Programming}{chapter.2}% 18
\BOOKMARK [2][-]{subsection.2.2.1}{Dynamic Programming Shortcomings}{section.2.2}% 19
\BOOKMARK [2][-]{subsection.2.2.2}{Generalized Policy Iteration}{section.2.2}% 20
\BOOKMARK [1][-]{section.2.3}{Monte Carlo Methods}{chapter.2}% 21
\BOOKMARK [1][-]{section.2.4}{Temporal Difference Methods}{chapter.2}% 22
\BOOKMARK [2][-]{subsection.2.4.1}{SARSA}{section.2.4}% 23
\BOOKMARK [2][-]{subsection.2.4.2}{Q-Learning}{section.2.4}% 24
\BOOKMARK [0][-]{chapter.3}{Deep Neural Networks}{}% 25
\BOOKMARK [1][-]{section.3.1}{Why Neural Networks?}{chapter.3}% 26
\BOOKMARK [1][-]{section.3.2}{Building Units}{chapter.3}% 27
\BOOKMARK [2][-]{subsection.3.2.1}{Artificial Neuron}{section.3.2}% 28
\BOOKMARK [2][-]{subsection.3.2.2}{Activation Functions}{section.3.2}% 29
\BOOKMARK [1][-]{section.3.3}{Feed Forward Networks}{chapter.3}% 30
\BOOKMARK [2][-]{subsection.3.3.1}{Training the Network}{section.3.3}% 31
\BOOKMARK [2][-]{subsection.3.3.2}{Underfitting and Overfitting}{section.3.3}% 32
\BOOKMARK [1][-]{section.3.4}{Convolutional Neural Networks}{chapter.3}% 33
\BOOKMARK [2][-]{subsection.3.4.1}{CNN Architecture}{section.3.4}% 34
\BOOKMARK [2][-]{subsection.3.4.2}{Applications}{section.3.4}% 35
\BOOKMARK [0][-]{chapter.4}{Deep Reinforcement Learning}{}% 36
\BOOKMARK [1][-]{section.4.1}{Introduction}{chapter.4}% 37
\BOOKMARK [1][-]{section.4.2}{Deep Q-Learning}{chapter.4}% 38
\BOOKMARK [2][-]{subsection.4.2.1}{DQN shortcomings}{section.4.2}% 39
\BOOKMARK [0][-]{chapter.5}{Cart-Pole Balancing with Visual Input}{}% 40
\BOOKMARK [1][-]{section.5.1}{Why Cart-Pole Balancing?}{chapter.5}% 41
\BOOKMARK [1][-]{section.5.2}{Experimental Setup and Tools}{chapter.5}% 42
\BOOKMARK [2][-]{subsection.5.2.1}{Hardware and Software}{section.5.2}% 43
\BOOKMARK [2][-]{subsection.5.2.2}{PyTorch}{section.5.2}% 44
\BOOKMARK [2][-]{subsection.5.2.3}{OpenAI Gym}{section.5.2}% 45
\BOOKMARK [2][-]{subsection.5.2.4}{The CartPole Environment}{section.5.2}% 46
\BOOKMARK [1][-]{section.5.3}{Solving the Sensor-Based CartPole}{chapter.5}% 47
\BOOKMARK [1][-]{section.5.4}{Towards Vision-Based Control}{chapter.5}% 48
\BOOKMARK [2][-]{subsection.5.4.1}{Improving Image Pre-Processing}{section.5.4}% 49
\BOOKMARK [2][-]{subsection.5.4.2}{Dealing with Overfitting}{section.5.4}% 50
\BOOKMARK [2][-]{subsection.5.4.3}{Reward Function Design}{section.5.4}% 51
\BOOKMARK [1][-]{section.5.5}{Vision-Based Control}{chapter.5}% 52
\BOOKMARK [2][-]{subsection.5.5.1}{Convolutional Neural Network Design}{section.5.5}% 53
\BOOKMARK [2][-]{subsection.5.5.2}{Hyper-Parameters Settings}{section.5.5}% 54
\BOOKMARK [2][-]{subsection.5.5.3}{Final Results}{section.5.5}% 55
\BOOKMARK [0][-]{chapter.6}{Conclusions and Future Work}{}% 56
\BOOKMARK [1][-]{section.6.1}{Thesis Summary}{chapter.6}% 57
\BOOKMARK [1][-]{section.6.2}{Future Work}{chapter.6}% 58
\BOOKMARK [0][-]{appendix.A}{Additional Reinforcement Learning Algorithms}{}% 59
\BOOKMARK [1][-]{section.A.1}{Model-Free Methods}{appendix.A}% 60
\BOOKMARK [2][-]{subsection.A.1.1}{TD\(\)}{section.A.1}% 61
\BOOKMARK [2][-]{subsection.A.1.2}{Policy Search Methods}{section.A.1}% 62
\BOOKMARK [2][-]{subsection.A.1.3}{Actor Critic Methods}{section.A.1}% 63
\BOOKMARK [2][-]{subsection.A.1.4}{Deterministic Policy Gradient}{section.A.1}% 64
\BOOKMARK [1][-]{section.A.2}{Model-based Methods}{appendix.A}% 65
\BOOKMARK [0][-]{appendix.B}{Additional Deep Reinforcement Learning Algorithms}{}% 66
\BOOKMARK [1][-]{section.B.1}{Deep Deterministic Policy Gradient}{appendix.B}% 67
\BOOKMARK [1][-]{section.B.2}{Trust Region Policy Optimization}{appendix.B}% 68
\BOOKMARK [1][-]{section.B.3}{Other Deep RL Algorithms}{appendix.B}% 69
\BOOKMARK [2][-]{subsection.B.3.1}{Generalized Advantage Estimation}{section.B.3}% 70
\BOOKMARK [2][-]{subsection.B.3.2}{Guided Policy Search}{section.B.3}% 71
\BOOKMARK [1][-]{section.B.4}{Deep RL Algorithms Comparison}{appendix.B}% 72
