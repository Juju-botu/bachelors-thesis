\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{2}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}What is Reinforcement Learning?}{2}{section.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Machine Learning}{3}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Supervised Learning}{3}{subsection.1.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Machine learning categories}}{4}{figure.1.1}}
\newlabel{fig:mlcategories}{{1.1}{4}{Machine learning categories}{figure.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Unsupervised Learning}{4}{subsection.1.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}How is Reinforcement Learning different?}{5}{subsection.1.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Elements of Reinforcement Learning}{5}{section.1.3}}
\newlabel{sec:elementsRL}{{1.3}{5}{Elements of Reinforcement Learning}{section.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Thesis outline}{7}{section.1.4}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}The Reinforcement Learning problem}{8}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:rl}{{2}{8}{The Reinforcement Learning problem}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Markov Decision Processes (MDP)}{8}{section.2.1}}
\newlabel{sec:MDP}{{2.1}{8}{Markov Decision Processes (MDP)}{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}The Agent-Environment Interface}{8}{subsection.2.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The Agent-Environment interface}}{9}{figure.2.1}}
\newlabel{fig:agentenvironment}{{2.1}{9}{The Agent-Environment interface}{figure.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}The Markov Property}{9}{subsection.2.1.2}}
\newlabel{sec:markov}{{2.1.2}{9}{The Markov Property}{subsection.2.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Returns and Rewarding Process}{9}{subsection.2.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Policies and Value Functions}{10}{subsection.2.1.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Bellman Equations}{11}{subsection.2.1.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.6}Optimality Conditions}{11}{subsection.2.1.6}}
\citation{bertsekas1995dynamic}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.7}Exploration and Exploitation}{12}{subsection.2.1.7}}
\newlabel{sec:explorationExploitation}{{2.1.7}{12}{Exploration and Exploitation}{subsection.2.1.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Dynamic Programming}{12}{section.2.2}}
\citation{metropolis1949monte}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Dynamic Programming Shortcomings}{13}{subsection.2.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Generalized Policy Iteration}{13}{subsection.2.2.2}}
\newlabel{sec:gpi}{{2.2.2}{13}{Generalized Policy Iteration}{subsection.2.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Monte Carlo Methods}{14}{section.2.3}}
\newlabel{eq:MC}{{2.13}{14}{Monte Carlo Methods}{equation.2.3.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Temporal Difference Methods}{14}{section.2.4}}
\newlabel{eq:TD}{{2.14}{15}{Temporal Difference Methods}{equation.2.4.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}SARSA}{15}{subsection.2.4.1}}
\citation{watkins1992q}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces SARSA}}{16}{algocf.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Q-Learning}{16}{subsection.2.4.2}}
\newlabel{sec:qlearning}{{2.4.2}{16}{Q-Learning}{subsection.2.4.2}{}}
\newlabel{alg:Qlearning}{{2}{16}{Q-Learning}{algocfline.2}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Q-learning}}{16}{algocf.2}}
\newlabel{eq:Qlearning}{{2.17}{16}{Q-Learning}{equation.2.4.17}{}}
\citation{silver2017mastering}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Deep Neural Networks}{18}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:DNN}{{3}{18}{Deep Neural Networks}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Why Neural Networks?}{18}{section.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Building Units}{18}{section.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces An artificial neuron}}{19}{figure.3.1}}
\newlabel{fig:neuron}{{3.1}{19}{An artificial neuron}{figure.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Artificial Neuron}{19}{subsection.3.2.1}}
\newlabel{eq:neuron}{{3.1}{19}{Artificial Neuron}{equation.3.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Activation Functions}{19}{subsection.3.2.2}}
\newlabel{subsec:ActivationFunctions}{{3.2.2}{19}{Activation Functions}{subsection.3.2.2}{}}
\citation{nair2010rectified}
\newlabel{eq:ReLu}{{3.4}{20}{Activation Functions}{equation.3.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Feed Forward Networks}{20}{section.3.3}}
\newlabel{sec:feedforward}{{3.3}{20}{Feed Forward Networks}{section.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces A Feed Forward neural network with two hidden layers}}{20}{figure.3.2}}
\newlabel{fig:feedforward}{{3.2}{20}{A Feed Forward neural network with two hidden layers}{figure.3.2}{}}
\citation{kingma2014adam}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Training the Network}{21}{subsection.3.3.1}}
\newlabel{subsec:traininnet}{{3.3.1}{21}{Training the Network}{subsection.3.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Underfitting and Overfitting}{22}{subsection.3.3.2}}
\newlabel{sec:overfitting}{{3.3.2}{22}{Underfitting and Overfitting}{subsection.3.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Examples of underfitted, good fit and overfitted networks}}{22}{figure.3.3}}
\newlabel{fig:fitting}{{3.3}{22}{Examples of underfitted, good fit and overfitted networks}{figure.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Convolutional Neural Networks}{22}{section.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}CNN Architecture}{23}{subsection.3.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces A CNN for recognizing hand-written digits}}{23}{figure.3.4}}
\newlabel{fig:CNN}{{3.4}{23}{A CNN for recognizing hand-written digits}{figure.3.4}{}}
\citation{nair2010rectified}
\citation{srivastava2014dropout}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Applications}{24}{subsection.3.4.2}}
\citation{mnih2013playing}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Deep Reinforcement Learning}{25}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:deeprl}{{4}{25}{Deep Reinforcement Learning}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{25}{section.4.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Deep Q-Learning}{25}{section.4.2}}
\newlabel{sec:DQN}{{4.2}{25}{Deep Q-Learning}{section.4.2}{}}
\newlabel{eq:lossfunction}{{4.1}{25}{Deep Q-Learning}{equation.4.2.1}{}}
\citation{schaul2015prioritized}
\citation{mnih2013playing}
\newlabel{eq:gradientloss}{{4.2}{26}{Deep Q-Learning}{equation.4.2.2}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces Deep Q-Learning with Experience Replay}}{27}{algocf.3}}
\newlabel{alg:DQN}{{3}{27}{Deep Q-Learning}{algocf.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}DQN shortcomings}{27}{subsection.4.2.1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Cart-Pole Balancing with Visual Input}{28}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:CartPole}{{5}{28}{Cart-Pole Balancing with Visual Input}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Why Cart-Pole Balancing?}{28}{section.5.1}}
\newlabel{sec:whyCartPole}{{5.1}{28}{Why Cart-Pole Balancing?}{section.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Experimental Setup and Tools}{29}{section.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Hardware and Software}{29}{subsection.5.2.1}}
\newlabel{sec:hardwaresoftware}{{5.2.1}{29}{Hardware and Software}{subsection.5.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}PyTorch}{29}{subsection.5.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}OpenAI Gym}{30}{subsection.5.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}The CartPole Environment}{30}{subsection.5.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Screenshot of the CartPole environment}}{31}{figure.5.1}}
\newlabel{fig:CartPoleScreenshot}{{5.1}{31}{Screenshot of the CartPole environment}{figure.5.1}{}}
\newlabel{table:tableenvironment}{{5.2.4}{31}{The CartPole Environment}{figure.5.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Table showing the CartPole's observation space}}{31}{table.5.1}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Solving the Sensor-Based CartPole}{32}{section.5.3}}
\newlabel{sec:simpleCartPole}{{5.3}{32}{Solving the Sensor-Based CartPole}{section.5.3}{}}
\newlabel{fig:easyDQN}{{5.3}{32}{Solving the Sensor-Based CartPole}{section.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Neural network with four input neurons for the states, and two output neurons for predicting the actions values}}{32}{figure.5.2}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Towards Vision-Based Control}{33}{section.5.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Oscillatory behavior of learning due to the vanishing gradients issue, showing no learning convergence. The orange line shows the mean of the last 100 episodes.}}{34}{figure.5.3}}
\newlabel{fig:vanishingGradients}{{5.3}{34}{Oscillatory behavior of learning due to the vanishing gradients issue, showing no learning convergence. The orange line shows the mean of the last 100 episodes}{figure.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Improving Image Pre-Processing}{35}{subsection.5.4.1}}
\newlabel{fig:ExampleExtractScreen}{{5.4.1}{35}{Improving Image Pre-Processing}{subsection.5.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Extracted screen from the CartPole environment: the image is extracted by cropping, downsampling and applying grayscale. This is the only state input for the DQN algorithm.}}{35}{figure.5.4}}
\citation{srivastava2014dropout}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Dealing with Overfitting}{36}{subsection.5.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.3}Reward Function Design}{36}{subsection.5.4.3}}
\newlabel{eq:reward}{{5.2}{37}{Reward Function Design}{equation.5.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Vision-Based Control}{37}{section.5.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Convolutional Neural Network Design}{37}{subsection.5.5.1}}
\newlabel{table:NeuralNetwork}{{5.5.1}{38}{Convolutional Neural Network Design}{subsection.5.5.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Convolutional Neural Network hyper-parameters}}{38}{table.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Hyper-Parameters Settings}{38}{subsection.5.5.2}}
\newlabel{table:hyperParameters}{{5.5.2}{38}{Hyper-Parameters Settings}{subsection.5.5.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Reinforcement Learning hyper-parameters}}{38}{table.5.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.3}Final Results}{39}{subsection.5.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Graph showing CartPole balancing score with respect to an over 800 episodes time span, with training beginning from scratch. Winning has been achieved at episode 659, scoring more than the 195 mean reward over the last 100 episodes required for success. At episode 723, the average reward of 200/200 was reached in this training session.}}{40}{figure.5.5}}
\newlabel{fig:CartPoleWon}{{5.5}{40}{Graph showing CartPole balancing score with respect to an over 800 episodes time span, with training beginning from scratch. Winning has been achieved at episode 659, scoring more than the 195 mean reward over the last 100 episodes required for success. At episode 723, the average reward of 200/200 was reached in this training session}{figure.5.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusions and Future Work}{41}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:conclusion}{{6}{41}{Conclusions and Future Work}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Thesis Summary}{41}{section.6.1}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Future Work}{42}{section.6.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Additional Reinforcement Learning Algorithms}{44}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{appendixA}{{A}{44}{Additional Reinforcement Learning Algorithms}{appendix.A}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Model-Free Methods}{44}{section.A.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.1}TD($\lambda $)}{44}{subsection.A.1.1}}
\citation{konda2000actor}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.2}Policy Search Methods}{45}{subsection.A.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.3}Actor Critic Methods}{46}{subsection.A.1.3}}
\newlabel{sec:actorcritic}{{A.1.3}{46}{Actor Critic Methods}{subsection.A.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.4}Deterministic Policy Gradient}{46}{subsection.A.1.4}}
\newlabel{eq:DPG}{{A.5}{46}{Deterministic Policy Gradient}{equation.A.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Model-based Methods}{47}{section.A.2}}
\citation{lillicrap2015continuous}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Additional Deep Reinforcement Learning Algorithms}{48}{appendix.B}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{appendixB}{{B}{48}{Additional Deep Reinforcement Learning Algorithms}{appendix.B}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Deep Deterministic Policy Gradient}{48}{section.B.1}}
\newlabel{sec:DDPG}{{B.1}{48}{Deep Deterministic Policy Gradient}{section.B.1}{}}
\newlabel{eq:criticupdate}{{B.1}{48}{Deep Deterministic Policy Gradient}{equation.B.1.1}{}}
\citation{schulman2015trust}
\newlabel{eq:lossactorcritic}{{B.2}{49}{Deep Deterministic Policy Gradient}{equation.B.1.2}{}}
\newlabel{eq:DPG_actorcritic_gradient}{{B.3}{49}{Deep Deterministic Policy Gradient}{equation.B.1.3}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces Deep Deterministic Policy Gradient}}{49}{algocf.4}}
\newlabel{alg:DDPG}{{4}{49}{Deep Deterministic Policy Gradient}{algocf.4}{}}
\citation{hershey2007approximating}
\citation{schulman2015high}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Trust Region Policy Optimization}{50}{section.B.2}}
\newlabel{eq:TRPO}{{B.5}{50}{Trust Region Policy Optimization}{equation.B.2.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.3}Other Deep RL Algorithms}{50}{section.B.3}}
\citation{levine2013guided}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3.1}Generalized Advantage Estimation}{51}{subsection.B.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3.2}Guided Policy Search}{51}{subsection.B.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {B.4}Deep RL Algorithms Comparison}{51}{section.B.4}}
\bibstyle{ieetr}
\citation{*}
\bibdata{chapters/bibliography}
\bibcite{bertsekas1995dynamic}{1}
\bibcite{metropolis1949monte}{2}
\bibcite{watkins1992q}{3}
\bibcite{silver2017mastering}{4}
\bibcite{nair2010rectified}{5}
\bibcite{kingma2014adam}{6}
\bibcite{srivastava2014dropout}{7}
\bibcite{mnih2013playing}{8}
\bibcite{schaul2015prioritized}{9}
\bibcite{konda2000actor}{10}
\bibcite{lillicrap2015continuous}{11}
\bibcite{schulman2015trust}{12}
\bibcite{hershey2007approximating}{13}
\bibcite{schulman2015high}{14}
\bibcite{levine2013guided}{15}
\bibcite{sutton1998introduction}{16}
\bibcite{goodfellow2016deep}{17}
\bibcite{barto1994reinforcement}{18}
\bibcite{vemulavision}{19}
\bibcite{rastogi2017deep}{20}
\bibcite{lample2017playing}{21}
\bibcite{appiahplaying}{22}
\bibcite{arulkumaran2017deep}{23}
\bibcite{quillen2018deep}{24}
\bibcite{nichols2014reinforcement}{25}
\bibcite{zhang2015towards}{26}
\bibcite{chen2018comparing}{27}
\bibcite{gu2016continuous}{28}
\bibcite{hochlander2014deep}{29}
